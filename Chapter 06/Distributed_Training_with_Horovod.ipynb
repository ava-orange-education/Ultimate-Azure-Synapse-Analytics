{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%configure -f\r\n",
        "{\r\n",
        "    \"driverMemory\": \"30g\",\r\n",
        "    \"driverCores\": 4,\r\n",
        "    \"executorMemory\": \"60g\",\r\n",
        "    \"executorCores\": 12,\r\n",
        "    \"numExecutors\": 3,\r\n",
        "    \"conf\":{\r\n",
        "        \"spark.rapids.memory.gpu.reserve\": \"10g\",\r\n",
        "        \"spark.executorEnv.TF_FORCE_GPU_ALLOW_GROWTH\": \"true\",\r\n",
        "        \"spark.kryoserializer.buffer.max\": \"2000m\"\r\n",
        "   }\r\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# base libs\r\n",
        "import sys\r\n",
        "import uuid\r\n",
        "import numpy as np\r\n",
        "# pyspark related\r\n",
        "import pyspark\r\n",
        "import pyspark.sql.types as T\r\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.sql.functions import udf\r\n",
        "# pytorch related\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "# horovod related\r\n",
        "import horovod.spark.torch as hvd\r\n",
        "from horovod.spark.common.backend import SparkBackend\r\n",
        "from horovod.spark.common.store import Store\r\n",
        "# azure related\r\n",
        "from azure.synapse.ml.horovodutils import AdlsStore"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize SparkSession\r\n",
        "spark = SparkSession.builder.getOrCreate()\r\n",
        "# Download MNIST dataset from Azure Open Datasets\r\n",
        "from azureml.opendatasets import MNIST\r\n",
        "mnist = MNIST.get_tabular_dataset()\r\n",
        "mnist_df = mnist.to_pandas_dataframe()\r\n",
        "mnist_df.info()\r\n",
        "# Preprocess dataset\r\n",
        "mnist_df['features'] = mnist_df.iloc[:, :784].values.tolist()\r\n",
        "mnist_df.drop(mnist_df.iloc[:, :784], inplace=True, axis=1)\r\n",
        "mnist_df.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Spark DataFrame for training\r\n",
        "df = spark.createDataFrame(mnist_df)\r\n",
        "\r\n",
        "# repartition DataFrame for training\r\n",
        "train_df = df.repartition(num_proc)\r\n",
        "\r\n",
        "# Train/test split\r\n",
        "train_df, test_df = train_df.randomSplit([0.9, 0.1])\r\n",
        "\r\n",
        "# show the dataset\r\n",
        "train_df.show()\r\n",
        "train_df.count()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the PyTorch model without any Horovod-specific parameters\r\n",
        "class Net(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self):\r\n",
        "        super(Net, self).__init__()\r\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\r\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\r\n",
        "        self.conv2_drop = nn.Dropout2d()\r\n",
        "        self.fc1 = nn.Linear(320, 50)\r\n",
        "        self.fc2 = nn.Linear(50, 10)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = x.float()\r\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\r\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\r\n",
        "        x = x.view(-1, 320)\r\n",
        "        x = F.relu(self.fc1(x))\r\n",
        "        x = F.dropout(x, training=self.training)\r\n",
        "        x = self.fc2(x)\r\n",
        "        return F.log_softmax(x)\r\n",
        "\r\n",
        "\r\n",
        "model = Net()\r\n",
        "optimizer = optim.SGD(model.parameters(),\r\n",
        "                      lr=lr_single_node * num_proc,\r\n",
        "                      momentum=0.5)  # notice the lr is scaled up\r\n",
        "loss = nn.NLLLoss()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Horovod Spark Estimator on the DataFrame\r\n",
        "backend = SparkBackend(num_proc=num_proc,\r\n",
        "                       stdout=sys.stdout,\r\n",
        "                       stderr=sys.stderr,\r\n",
        "                       prefix_output_with_timestamp=True)\r\n",
        "\r\n",
        "torch_estimator = hvd.TorchEstimator(\r\n",
        "    backend=backend,\r\n",
        "    store=store,\r\n",
        "    partitions_per_process=1,  # important for GPU training\r\n",
        "    model=model,\r\n",
        "    optimizer=optimizer,\r\n",
        "    loss=lambda input, target: loss(input, target.long()),\r\n",
        "    input_shapes=[[-1, 1, 28, 28]],\r\n",
        "    feature_cols=['features'],\r\n",
        "    label_cols=['label'],\r\n",
        "    batch_size=batch_size,\r\n",
        "    epochs=epochs,\r\n",
        "    validation=0.1,\r\n",
        "    verbose=2)\r\n",
        "\r\n",
        "torch_model = torch_estimator.fit(train_df).setOutputCols(['label_prob'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the held-out test DataFrame\r\n",
        "pred_df = torch_model.transform(test_df)\r\n",
        "\r\n",
        "argmax = udf(lambda v: float(np.argmax(v)), returnType=T.DoubleType())\r\n",
        "pred_df = pred_df.withColumn('label_pred', argmax(pred_df.label_prob))\r\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol='label_pred',\r\n",
        "                                              labelCol='label',\r\n",
        "                                              metricName='accuracy')\r\n",
        "\r\n",
        "print('Test accuracy:', evaluator.evaluate(pred_df))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}